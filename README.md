# Datalake Solution with AWS EMR and Apache Spark

Tech Stack: AWS EMR, S3, Python, SQL, Pyspark.

This project is a cloud datalake solution for a popular music streaming app to store, analyze and gather insight from its user activity data.The project aims to understand what songs users are listening to. The company has user activity data in JSON format stored in an AWS S3 bucket and want to build a reliable a datalake solution and harness the power of spark to load and transform the data in memory and save the results in parquet files in a S3 bucket. The `etl.py` script is meant to run in a EMR notebook within the AWS environment.

## AWS EMR Cluster Configuration

- Software Configuration
-- Release: emr-5.30.0
-- Applications: Spark: Spark 2.3.0 on Haddoop 2.8.5 YARN with Gangilia 3.7.3 and Zeppelin 0.8.0
- Hardware Configuration
-- Instance Type: m3.xlarge
-- Number of Instance: 3

## Source Data

The source Data for this project are two sets of JSON files that contains information realted to the songs and user play history respectively.

#### Song JSON files
Song files are partitioned by the first three letters of each song's track ID and are a subset of real data from the [Million Song Dataset](http://millionsongdataset.com/).  The following filepath and its content are given as an example:

> song_data/A/A/B/TRAABJL12903CDCF1A.json

> {"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "", "artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}

#### Log JSON files

Log files are partitioned by year and month as follows and is generated by this [event simulator](https://github.com/Interana/eventsim) based on the songs in the dataset above. These simulate app activity logs from a music streaming app based on specified configurations.:

> log_data/2018/11/2018-11-13-events.json


## Table Schema

#### Fact Table

| songplays |
| --- |
| songplay_id |
| start_time |
| user_id |
| level |
| song_id |
| artist_id |
| session_id |
| location |
| user_agent |
> Records in log data associated with song plays i.e. records with page

#### Dimension Tables

| users  |
| --- |
| user_id |
| first_name |
| last_name |
| gender |
| level |
> App users 

| songs   |
| --- |
| song_id |
| title |
| artist_id |
| year |
| duration |
> Songs in music database

| artists    |
| --- |
| artist_id |
| name |
| location |
| lattitude |
| longitude |
> Artists in music database

| time     |
| --- |
| start_time |
| hour |
| day |
| week |
| month |
| year |
| weekday |

## File descriptions

`etl.py` initiates a spark instance, load the raw json files from source s3 bucket, perform a set of dataframe transformations in memory and save the result dataframe in to a destination s3 bucket.

## To Run

Change output_data variable in main() to a valid s3 bucket address of your choosing.
